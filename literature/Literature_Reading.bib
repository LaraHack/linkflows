% This file was created with JabRef 2.10.
% Encoding: UTF8


@Article{Andel1994,
  Title                    = {Anatomy of the unsought finding. Serendipity: origin, history, domains, traditions, appearances, patterns and programmability},
  Author                   = {Pek Van Andel},
  Journal                  = {The British Journal for the Philosophy of Science},
  Year                     = {1994},

  Month                    = {June},
  Number                   = {2},
  Pages                    = {631-648},
  Volume                   = {45},

  Abstract                 = {I define serendipity as the art of making an unsought finding. And I propose an overview of my collection of serendipities, the largest yet assembled, chiefly in science and technology, but also in art, by giving a list of 'serendipity patterns'. Although my list of 'patterns' is just a list and not a classification, it serves to introduce a new and possibly stimulating perspective on the old subject of serendipity. Knowledge of these 'serendipity patterns' ight help in expecting also the unexpected and in finding also the unsought.},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Anatomy_of_the_Unsought_Finding.Serendipity\:Orgin%2CHistory%2CDomains%2CTraditions%2CAppearances%2CPatterns_and_Programmability-vanAcker.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Anatomy_of_the_Unsought_Finding.Serendipity\:Orgin,History,Domains,Traditions,Appearances,Patterns_and_Programmability-vanAcker.pdf:PDF},
  Keywords                 = {reasoning, inductive reasoning, serendipity},
  Url                      = {http://www.jstor.org/stable/687687}
}

@Inproceedings{Augenstein2014,
  Title                    = {Joint Information Extraction from the Web using Linked Data},
  Author                   = {Isabelle Augenstein},
  Booktitle                = {Proceedings of the 13th International Semantic Web Conference Part II, Lecture Notes in Computer Series},
  Year                     = {2014},

  Address                  = {New York, NY, USA},
  Editor                   = {Peter Mika and Tania Tudorache and Abraham Bernstein and Chris Welty and Craig Knoblock and Denny Vrande\v{c}i\'{c} and Paul Groth and Natasha Noy and Krzysztof Janowicz and Carole Goble},
  Location                 = {Riva del Garda, Italy},
  Month                    = {October},
  Organization             = {ISWC: International Semantic Web Conference},
  Pages                    = {505-512},
  Publisher                = {Springer-Verlag},
  Series                   = {ISWC'14},
  Volume                   = {8797},

  Abstract                 = {Almost all of the big name Web companies are currently engaged in building ‘knowledge graphs’ and these are showing significant results in improving search, email, calendaring, etc. Even the largest openly-accessible ones, such as Freebase and Wikidata, are far from complete, partly because new information is emerging so quickly. Most of the missing information is available on Web pages. To access that knowledge and populate knowledge bases, information extraction methods are necessitated. The bottleneck for information extraction systems is obtaining training data to learn classifiers. In this doctoral research, we investigate how existing data in knowledge bases can be used to automatically annotate training data to learn classifiers to in turn extract more data to expand knowledge bases. We discuss our hypotheses, approach, evaluation methods and present preliminary results.},
  Doi                      = {10.1007/978-3-319-11915-1_32},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Joint_Information_Extraction_from_the_Web_using_Linked_Data-Augenstein.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Joint_Information_Extraction_from_the_Web_using_Linked_Data-Augenstein.pdf:PDF},
  Url                      = {https://link.springer.com/content/pdf/10.1007%2F978-3-319-11915-1_32.pdf}
}

@Inproceedings{Barati2016,
  Title                    = {SWARM: An Approach for Mining Semantic Association Rules from Semantic Web Data},
  Author                   = {Molood Barati and Quan Bai and Qing Liu},
  Booktitle                = {Trends in Artificial Intelligence, Lecture Notes in Computer Science},
  Year                     = {2016},

  Address                  = {Cham, Switzerland},
  Editor                   = {Booth R. and Zhang ML.},
  Location                 = {Phuket, Thailand},
  Month                    = {August},
  Organization             = {PRICAI: Pacific Rim International Conference on Artificial Intelligence},
  Pages                    = {30-43},
  Publisher                = {Springer},
  Series                   = {PRICAI'16},
  Volume                   = {9810},

  Abstract                 = {The ever growing amount of Semantic Web data has made it increasingly difficult to analyse the information required by the users. Association rule mining is one of the most useful techniques for discovering frequent patterns among RDF triples. In this context, some statistical methods strongly rely on the user intervention that is time-consuming and error-prone due to a large amount of data. In these studies, the rule quality factors (e.g. Support and Confidence measures) consider only knowledge in the instance-level data. However, Semantic Web data contains knowledge in both instance-level and schema-level. In this paper, we introduce an approach called SWARM (Semantic Web Association Rule Mining) to automatically mine Semantic Association Rules from RDF data. We discuss how to utilize knowledge encode in the schema-level to enrich the semantics of rules. We also show that our approach is able to reveal common behavioral patterns associated with knowledge in the instance-level and schema-level. The proposed rule quality factors (Support and Confidence) consider knowledge not only in the instance-level but also schema-level. Experiments performed on the DBpedia Dataset (3.8) demonstrate the usefulness of the proposed approach.},
  Doi                      = {10.1007/978-3-319-42911-3_3},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/SWARM-An_Approach_for_Mining_Semantic_Association_Rules_from_Semantic_Web_Data-Barati.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/SWARM-An_Approach_for_Mining_Semantic_Association_Rules_from_Semantic_Web_Data-Barati.pdf:PDF},
  Url                      = {https://link.springer.com/chapter/10.1007/978-3-319-42911-3_3}
}

@Inproceedings{Belhajjame2012,
  Title                    = {Workflowcentric research objects: First class citizens in scholarly discourse},
  Author                   = {Khalid Belhajjame and Oscar Corcho and Daniel Garijo and Jun Zhao and David Newman and Ra\'{u}l Palma and Sean Bechhofer and Esteban Garc\'{\i}a and Jos\'{e} Manuel G\'{o}mez-p\'{e}rez and Graham Klyne and Jos\'{e} Enrique Ruiz and Stian Soil and David De Roure and Carole A. Goble},
  Booktitle                = {Proceedings of the International Workshop on the Future of Scholary Communication and Scientific Publishing},
  Year                     = {2012},

  Address                  = {Aachen, Germany},
  Editor                   = {Frank Van Harmelen and Alexander Garc\'{\i}a Castro and Christoph Lange and Benjamin Good},
  Location                 = {Hersonissos, Crete, Greece},
  Month                    = {May},
  Organization             = {SePublica: Semantic Publishing},
  Pages                    = {1-12},
  Publisher                = {CEUR-WS.org},
  Series                   = {SePublica'12},
  Volume                   = {903},

  Abstract                 = {A workflow-centric research object bundles a workflow, the
provenance of the results obtained by its enactment, other digital objects
that are relevant for the experiment (papers, datasets, etc.), and anno-
tations that semantically describe all these objects. In this paper, we
propose a model to specify workflow-centric research objects, and show
how the model can be grounded using semantic technologies and exist-
ing vocabularies, in particular the Object Reuse and Exchange (ORE)
model and the Annotation Ontology (AO). We describe the life-cycle of a
research object, which resembles the life-cycle of a scientific experiment.},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Workflow-Centric_Research_Objects-Belhajjame.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Workflow-Centric_Research_Objects-Belhajjame.pdf:PDF},
  Url                      = {http://ceur-ws.org/Vol-903/paper-01.pdf}
}

@Inproceedings{Beutel2017,
  Title                    = {Beyond globally optimal: Focused Learning for improved recommendations},
  Author                   = {Alex Beutel and Ed H. Chi and Zhiyuan Cheng and Hubert Pham and John Anderson},
  Booktitle                = {Proceedings of the 26th International Conference on World Wide Web},
  Year                     = {2017},

  Address                  = {Republic and Canton of Geneva, Switzerland},
  Location                 = {Perth, Australia},
  Month                    = {April},
  Organization             = {WWW: World Wide Web Conference},
  Pages                    = {203-212},
  Publisher                = {International World Wide Web Conferences Steering Committee},
  Series                   = {WWW'17},

  Abstract                 = {When building a recommender system, how can we ensure
that all items are modeled well? Classically, recommender
systems are built, optimized, and tuned to improve a global
prediction objective, such as root mean squared error. How-
ever, as we demonstrate, these recommender systems of-
ten leave many items badly-modeled and thus under-served.
Further, we give both empirical and theoretical evidence
that no single matrix factorization, under current state-of-
the-art methods, gives optimal results for each item.
As a result, we ask: how can we learn additional mod-
els to improve the recommendation quality for a specified
subset of items? We offer a new technique called focused
learning, based on hyperparameter optimization and a cus-
tomized matrix factorization objective. Applying focused
learning on top of weighted matrix factorization, factoriza-
tion machines, and LLORMA, we demonstrate prediction
accuracy improvements on multiple datasets. For instance,
on MovieLens we achieve as much as a 17\% improvement in
prediction accuracy for niche movies, cold-start items, and
even the most badly-modeled items in the original model.},
  Doi                      = {10.1145/3038912.3052713},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Beyond_Globally_Optimal\:Focused_Learning_for_Improved_Recommendations-Beutel.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Beyond_Globally_Optimal\:Focused_Learning_for_Improved_Recommendations-Beutel.pdf:PDF},
  Url                      = {http://alexbeutel.com/papers/www2017_focused_learning.pdf}
}

@Article{Boyack2005,
  Title                    = {Mapping the backbone of science},
  Author                   = {Kevin W. Boyack and Richard Klavans and Katy B\"orner},
  Journal                  = {Scientometrics},
  Year                     = {2005},
  Number                   = {3},
  Pages                    = {351-374},
  Volume                   = {64},

  Abstract                 = {This paper presents a new map representing the structure of all of science, based on journal articles, including both the natural and social sciences. Similar to cartographic maps of our world, the map of science provides a bird’s eye view of today`s scientific landscape. It can be used to visually identify major areas of science, their size, similarity, and interconnectedness. In order to be useful, the map needs to be accurate on a local and on a global scale. While our recent work has focused on the former aspect,1 this paper summarizes results on how to achieve structural accuracy. Eight alternative measures of journal similarity were applied to a data set of 7,121 journals covering over 1 million documents in the combined Science Citation and Social Science Citation Indexes. For each journal similarity measure we generated two-dimensional spatial layouts using the force-directed graph layout tool, VxOrd. Next, mutual information values were calculated for each graph at different clustering levels to give a measure of structural accuracy for each map. The best co-citation and inter-citation maps according to local and structural accuracy were selected and are presented and characterized. These two maps are compared to establish robustness. The inter-citation map is then used to examine linkages between disciplines. Biochemistry appears as the most interdisciplinary discipline in science.},
  Doi                      = {10.1007/s11192-005-0255-6},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Mapping_the_Backbone_of_Science-Boyack.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Mapping_the_Backbone_of_Science-Boyack.pdf:PDF},
  Keywords                 = {Mapping science, maps of science, data organization, clustering, interlinking, similarity measures},
  Url                      = {https://link.springer.com/article/10.1007/s11192-005-0255-6}
}

@Inproceedings{Ceolin2016,
  Title                    = {Identifying and classifying uncertainty layers in Web document quality assessment},
  Author                   = {Davide Ceolin and Lora Aroyo and Julia Noordegraaf},
  Booktitle                = {Proceedings of the 12th International Workshop on Uncertainty Reasoning for the Semantic Web},
  Year                     = {2016},

  Address                  = {Aachen, Germany},
  Location                 = {Kobe, Japan},
  Organization             = {URSW: Uncertainty Reasoning for the Semantic Web},
  Pages                    = {61-64},
  Publisher                = {CEUR-WS.org},
  Series                   = {USRW'16},
  Volume                   = {1665},

  Abstract                 = {Assessing the quality of Web documents is crucial, but chal-
lenging. In this paper, we outline the different uncertainty bottlenecks
that such task implies, and we propose a strategy to tackle them.},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Identifying_and_Classifying_Uncertainty_Layers_in_Web_Document_Quality_Assessment-Ceolin.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Identifying_and_Classifying_Uncertainty_Layers_in_Web_Document_Quality_Assessment-Ceolin.pdf:PDF},
  Url                      = {http://ceur-ws.org/Vol-1665/paper6.pdf}
}

@Inproceedings{Ceolin2016a,
  Title                    = {Capturing the ineffable: collecting, analysing, and automating Web document quality assessments},
  Author                   = {Davide Ceolin and Julia Noordegraaf and Lora Aroyo},
  Booktitle                = {Proceedings of the 20th International Conference on Knowledge Engineering and Knowledge Management, Lecture Notes in Computer Science},
  Year                     = {2016},

  Address                  = {Cham, Switzerland},
  Editor                   = {Eva Blomqvist and Paolo Ciancarini and Francesco Poggi and Fabio Vitali},
  Location                 = {Bologna, Italy},
  Month                    = {November},
  Organization             = {EKAW: European Knowledge Acquisition Workshop},
  Pages                    = {83-97},
  Publisher                = {Springer},
  Series                   = {EKAW'16},
  Volume                   = {10024},

  Abstract                 = {Automatic estimation of the quality of Web documents is a
challenging task, especially because the definition of quality heavily de-
pends on the individuals who define it, on the context where it applies,
and on the nature of the tasks at hand. Our long-term goal is to allow
automatic assessment of Web document quality tailored to specific user
requirements and context. This process relies on the possibility to iden-
tify document characteristics that indicate their quality. In this paper,
we investigate these characteristics as follows: (1) we define features of
Web documents that may be indicators of quality; (2) we design a pro-
cedure for automatically extracting those features; (3) develop a Web
application to present these results to niche users to check the relevance
of these features as quality indicators and collect quality assessments;
(4) we analyse user’s qualitative assessment of Web documents to re-
fine our definition of the features that determine quality, and establish
their relevant weight in the overall quality, i.e., in the summarizing score
users attribute to a document, determining whether it meets their stan-
dards or not. Hence, our contribution is threefold: a Web application for
nichesourcing quality assessments; a curated dataset of Web document
assessments; and a thorough analysis of the quality assessments collected
by means of two case studies involving experts (journalists and media
scholars). The dataset obtained is limited in size but highly valuable be-
cause of the quality of the experts that provided it. Our analyses show
that: (1) it is possible to automate the process of Web document quality
estimation to a level of high accuracy; (2) document features shown in
isolation are poorly informative to users; and (3) related to the tasks
we propose (i.e., choosing Web documents to use as a source for writ-
ing an article on the vaccination debate), the most important quality
dimensions are accuracy, trustworthiness, and precision.},
  Doi                      = {10.1007/978-3-319-49004-5_6},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Capturing_the_Ineffable\:Collecting%2CAnalysing_and_Automating_Web_Document_Quality_Assessments-Ceolin.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Capturing_the_Ineffable\:Collecting,Analysing_and_Automating_Web_Document_Quality_Assessments-Ceolin.pdf:PDF},
  Url                      = {link.springer.com/content/pdf/10.1007%2F978-3-319-49004-5_6.pdf}
}

@Inproceedings{Ceolin2016b,
  Title                    = {Towards Web documents quality assessment for digital humanities scholars},
  Author                   = {Davide Ceolin and Julia Noordegraaf and Lora Aroyo and Chantal van Son},
  Booktitle                = {Proceedings of the 8th ACM Conference on Web Science},
  Year                     = {2016},

  Address                  = {New York, NY, USA},
  Editor                   = {Wolfgang Nejdl and Wendy Hall and Paolo Parigi and Steffen Staab},
  Location                 = {Hannover, Germany},
  Month                    = {May},
  Organization             = {WebSci: Web Science},
  Pages                    = {315-317},
  Publisher                = {ACM},
  Series                   = {WebSci'16},

  Abstract                 = {We present a framework for assessing the quality of Web documents, and a baseline of three quality dimensions: trustworthiness, objectivity and basic scholarly quality. Assessing Web document quality is a "deep data" problem necessitating approaches to handle both data size and complexity.},
  Doi                      = {10.1145/2908131.2908198},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Towards_Web_Documents_Quality_Assessment_for_Digital_Humanities_Scholars-Ceolin.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Towards_Web_Documents_Quality_Assessment_for_Digital_Humanities_Scholars-Ceolin.pdf:PDF},
  Url                      = {http://delivery.acm.org/10.1145/2910000/2908198/p315-ceolin.pdf}
}

@Inproceedings{Ciancarini2013,
  Title                    = {Semantic annotation of scholarly documents and citations},
  Author                   = {Paolo Ciancarini and Angelo Di Iorio and Andrea Giovanni Nuzzolese and Silvio Peroni and Fabio Vitali},
  Booktitle                = {Congress of the Italian Association for Artificial Intelligence, Advances in Artificial Intelligence, Lecture Notes in Computer Science},
  Year                     = {2013},

  Address                  = {Cham, Switzerland},
  Organization             = {AI*IA: Advances in Artificial Intelligence},
  Pages                    = {336-347},
  Publisher                = {Springer},
  Series                   = {AI*IA'13},
  Volume                   = {8249},

  Abstract                 = {Scholarly publishing is in the middle of a revolution based on the use of Web-related technologies as medium of communication. In this paper we describe our ongoing study of semantic publishing and automatic annotation of scholarly documents, presenting several models and tools for the automatic annotation of structural and semantic components of documents. In particular, we focus on citations and their automatic classification obtained by CiTalO, a framework that combines ontology learning techniques with NLP techniques.},
  Doi                      = {10.1007/978-3-319-03524-6_29},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Semantic_Annotation_of_Scholarly_Documents_and_Citations-Ciancarini.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Semantic_Annotation_of_Scholarly_Documents_and_Citations-Ciancarini.pdf:PDF},
  Url                      = {https://link.springer.com/chapter/10.1007/978-3-319-03524-6_29}
}

@Article{Ciccarese2014a,
  Title                    = {The Collections Ontology: Creating and handling collections in OWL 2 DL frameworks},
  Author                   = {Paolo Ciccarese and Silvio Peroni},
  Journal                  = {Semantic Web},
  Year                     = {2014},
  Number                   = {6},
  Pages                    = {515-529},
  Volume                   = {5},

  Abstract                 = {The RDF collections and containers is one of the most used features by RDF technicians and practitioners. Although some work has
been published in past, there is not a standard and accepted way for defining collections within OWL DL frameworks. Here, we
attempt to address this issue with the introduction of the Collections Ontology (CO) version 2.0. CO is an OWL 2 DL ontology
developed for creating sets, bags and lists of resources, and for inferring collection properties even in the presence of incomplete
information.},
  Doi                      = {10.3233/SW-130121},
  Url                      = {http://www.semantic-web-journal.net/system/files/swj432.pdf}
}

@Article{Ciccarese2014,
  Title                    = {CiTO + SWAN: The Web Semantics of Bibliographic Records, Citations, Evidence and Discourse Relationships},
  Author                   = {Paolo Ciccarese and David Shottonc and Silvio Peronic and Tim Clark},
  Journal                  = {Semantic Web},
  Year                     = {2014},

  Month                    = {October},
  Number                   = {4},
  Pages                    = {295-311},
  Volume                   = {5},

  Abstract                 = {Most literature searching in biomedicine is now conducted via PubMed, Google
Scholar or other web-based bibliographic search mechanisms. Yet until now a public, open,
interoperable and complete web-adapted information schema for bibliographic citations,
bibliographic references and scientific discourse has not been available. Such a schema,
expressed in the form of a description logic compatible with current web semantics approaches,
would provide the ability to treat bibliographic references and citations, and rhetorical discourse
in scientific publications, as semantic metadata on the web, with all the benefits that implies for
organization, search and mash-up of web-based scientific information.
In this paper we present CiTO + SWAN, a set of fully harmonized ontology modules resulting
from the harmonization of CiTO (the Citation Typing Ontology) with SWAN (Semantic Web
Applications in Neuromedicine), which we have developed by jointly adapting and evolving
version 1.6 of CiTO, the Citation Typing Ontology, and version 1.2 of the SWAN Scientific
Discourse Ontology (v1.2). The CiTO + SWAN model is specified in OWL 2 DL, is fully
modular, and inherently supports agent-based searching and mash-ups.
Through the harmonization activity presented here, and previous work that harmonized SWAN
with the SIOC (Semantically-Interlinked Online Communities) Ontology for describing blogs,
wikis and discussion groups, we have construct the basis of a powerful new web framework for
scientific communications.},
  Doi                      = {10.3233/SW-130098},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/CiTO%2BSWAN\:The_Web_Semantics_of_Bibliographic_Records%2C%20Citations%2CEvidence_and_Discourse_Relationships-Ciccarese.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/CiTO+SWAN\:The_Web_Semantics_of_Bibliographic_Records, Citations,Evidence_and_Discourse_Relationships-Ciccarese.pdf:PDF},
  Keywords                 = {ontologies},
  Url                      = {http://semantic-web-journal.net/sites/default/files/swj175_0.pdf}
}

@Article{Clark2014,
  Title                    = {Micropublications: a semantic model for claims, evidence, arguments and annotations in biomedical communications},
  Author                   = {Tim Clark and Paolo N. Ciccarese and Carole A. Goble},
  Journal                  = {Journal of Biomedical Semantics},
  Year                     = {2014},
  Number                   = {28},
  Volume                   = {5},

  Abstract                 = {Background: Scientific publications are documentary representations of defeasible arguments, supported by data
and repeatable methods. They are the essential mediating artifacts in the ecosystem of scientific communications.
The institutional “goal” of science is publishing results. The linear document publication format, dating from 1665,
has survived transition to the Web.
Intractable publication volumes; the difficulty of verifying evidence; and observed problems in evidence and
citation chains suggest a need for a web-friendly and machine-tractable model of scientific publications. This model
should support: digital summarization, evidence examination, challenge, verification and remix, and incremental
adoption. Such a model must be capable of expressing a broad spectrum of representational complexity, ranging
from minimal to maximal forms.
Results: The micropublications semantic model of scientific argument and evidence provides these features.
Micropublications support natural language statements; data; methods and materials specifications; discussion and
commentary; challenge and disagreement; as well as allowing many kinds of statement formalization.
The minimal form of a micropublication is a statement with its attribution. The maximal form is a statement with its
complete supporting argument, consisting of all relevant evidence, interpretations, discussion and challenges
brought forward in support of or opposition to it. Micropublications may be formalized and serialized in multiple
ways, including in RDF. They may be added to publications as stand-off metadata.
An OWL 2 vocabulary for micropublications is available at http://purl.org/mp. A discussion of this vocabulary along
with RDF examples from the case studies, appears as OWL Vocabulary and RDF Examples in Additional file 1.
Conclusion: Micropublications, because they model evidence and allow qualified, nuanced assertions, can play
essential roles in the scientific communications ecosystem in places where simpler, formalized and purely
statement-based models, such as the nanopublications model, will not be sufficient. At the same time they will add
significant value to, and are intentionally compatible with, statement-based formalizations.
We suggest that micropublications, generated by useful software tools supporting such activities as writing, editing,
reviewing, and discussion, will be of great value in improving the quality and tractability of biomedical
communications.},
  Doi                      = {10.1186/2041-1480-5-28},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Micropublications-Clark.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Micropublications-Clark.pdf:PDF},
  Keywords                 = {Semantic publishing, Micropublications},
  Url                      = {https://jbiomedsem.biomedcentral.com/articles/10.1186/2041-1480-5-28}
}

@Article{Constantin2016,
  Title                    = {The Document Components Ontology (DoCO)},
  Author                   = {Alexandru Constantin and Silvio Peroni and Steve Pettifer and David Shotton and Fabio Vitali},
  Journal                  = {Semantic Web},
  Year                     = {2016},

  Month                    = {February},
  Number                   = {2},
  Pages                    = {167-181},
  Volume                   = {7},

  Abstract                 = {The availability in machine-readable form of descriptions of the structure of documents, as well as of the document
discourse (e.g. the scientific discourse within scholarly articles), is crucial for facilitating semantic publishing and the overall
comprehension of documents by both users and machines. In this paper we introduce DoCO, the Document Components Ontology,
an OWL 2 DL ontology that provides a general-purpose structured vocabulary of document elements to describe both structural and
rhetorical document components in RDF. In addition to describing the formal description of the ontology, this paper showcases its
utility in practice in a variety of our own applications and other activities of the Semantic Publishing community that rely on DoCO
to annotate and retrieve document components of scholarly articles.},
  Doi                      = {10.3233/SW-150177},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/The_Document_Components_Ontology-DoCO-Constantin.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/The_Document_Components_Ontology-DoCO-Constantin.pdf:PDF},
  Keywords                 = {ontologies, SPAR ontologies},
  Url                      = {http://content.iospress.com/download/semantic-web/sw177?id=semantic-web%2Fsw177}
}

@Online{roo,
  Title                    = {Research Object Ontology},
  Url                      = {http://wf4ever.github.io/ro/2016-01-28/},

  Author                   = {Oscar Corcho and Graham Klyne and Khalid Belhajjame and Daniel Garijo and Esteban Garc\'{\i}a Cuesta and Raul Palma},
  Organization             = {Workflow4Ever},

  Abstract                 = {The Wf4Ever Research Object Model provides a vocabulary for the description of workflow-centric Research Objects: aggregations of resources relating to scientific workflows.},
  Addendum                 = {An ontology for Research Objects that can be used to describe aggregations for scientific workflows.},
  Keywords                 = {ontologies}
}

@Inproceedings{Debattista2014a,
  Title                    = {daQ, an ontology for dataset quality information},
  Author                   = {Jeremy Debattista and Christoph Lange and S\"{o}ren Auer},
  Booktitle                = {Proceedings of the Workshop on Linked Data on the Web},
  Year                     = {2014},

  Address                  = {Aachen, Germany},
  Editor                   = {Christian Bizer and Tom Heath and S\"{o}ren Auer and Tim Berners-Lee},
  Location                 = {Seoul, Korea},
  Month                    = {April},
  Organization             = {LDOW: Linked Data on the Web},
  Publisher                = {CEUR-WS.org},
  Series                   = {LDOW'14},
  Volume                   = {1184},

  Abstract                 = {Data quality is commonly defined as fitness for use. The problem
of identifying the quality of data is faced by many data consumers.
To make the task of finding good quality datasets more efficient, we
introduce the Dataset Quality Ontology (daQ). The daQ is a light-
weight, extensible vocabulary for attaching the results of quality
benchmarking of a linked open dataset to that dataset. We dis-
cuss the design considerations, give examples for extending daQ
by custom quality metrics, and present use cases such as browsing
datasets by quality. We also discuss how tools can use the daQ to
enable consumers find the right dataset for use.},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/daQ-An_Ontology_for_Dataset_Quality_Information-Debattista.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/daQ-An_Ontology_for_Dataset_Quality_Information-Debattista.pdf:PDF},
  Keywords                 = {ontologies},
  Url                      = {http://ceur-ws.org/Vol-1184/ldow2014_paper_09.pdf}
}

@Inproceedings{Debattista2014,
  Title                    = {Representing dataset quality metadata using multi-dimensional views},
  Author                   = {Jeremy Debattista and Christoph Lange and S\"oren Auer},
  Booktitle                = {International Conference on Semantic Systems},
  Year                     = {2014},

  Address                  = {New York, NY, USA},
  Location                 = {Leipzig, Germany},
  Month                    = {September},
  Organization             = {SEM: Semantic Systems},
  Pages                    = {92-99},
  Publisher                = {ACM},

  Abstract                 = {Data quality is commonly defined as fitness for use. The problem of identifying quality of data is faced by many data consumers. Data publishers often do not have the means to identify quality problems in their data. To make the task for both stakeholders easier, we have developed the Dataset Quality Ontology (daQ). daQ is a core vocabulary for representing the results of quality benchmarking of a linked dataset. It represents quality metadata as multi-dimensional and statistical observations using the Data Cube vocabulary. Quality metadata are organised as a self-contained graph, which can, e.g., be embedded into linked open datasets. We discuss the design considerations, give examples for extending daQ by custom quality metrics, and present use cases such as analysing data versions, browsing datasets by quality, and link identification. We finally discuss how data cube visualisation tools enable data publishers and consumers to analyse better the quality of their data.},
  Doi                      = {10.1145/2660517.2660525},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Representing_Dataset_Quality_Metadata_Using_Muti-dimensional_Views-Debattista.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Representing_Dataset_Quality_Metadata_Using_Muti-dimensional_Views-Debattista.pdf:PDF},
  Url                      = {dl.acm.org/ft_gateway.cfm?ftid=1497526&id=2660525}
}

@Article{Feigenbaum2007,
  Title                    = {The Semantic Web in action},
  Author                   = {Lee Feigenbaum and Ivan Herman and Tonya Hongsermeier and Eric Neumann and Susie Stephens},
  Journal                  = {Scientific American},
  Year                     = {2007},

  Month                    = {December},
  Number                   = {6},
  Pages                    = {64-71},
  Volume                   = {297},

  Abstract                 = {A wide variety of online Semantic Web applications are emerging, from Vodafone Live!'s mobile phone service to Boeing's system for coordinating the work of vendors. Scientific researchers are developing some of the most advanced applications, including a system that pinpoints genetic causes of heart disease and another system that reveals the early stages of influenza outbreaks. Companies and universities, working through the World Wide Web Consortium, are developing standards that are making the Semantic Web more accessible and easy to use.},
  Doi                      = {10.1038/scientificamerican1207-90},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/The_Semantic_Web_in_Action-Feigenbaum.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/The_Semantic_Web_in_Action-Feigenbaum.pdf:PDF},
  Url                      = {https://www.scientificamerican.com/article/semantic-web-in-actio/}
}

@PhdThesis{Fielding2000,
  Title                    = {Architectural Styles and the Design of Network-based Software Architectures},
  Author                   = {Roy Thomas Fielding},
  School                   = {Univeristy of California, Irvine},
  Year                     = {2000},

  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Architectural_Styles_and_the_Design_of_Network-based_Software_Architectures-Fielding.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Architectural_Styles_and_the_Design_of_Network-based_Software_Architectures-Fielding.pdf:PDF},
  Url                      = {https://www.ics.uci.edu/~fielding/pubs/dissertation/top.htm}
}

@Inproceedings{Fokkens2016,
  Title                    = {On the semantics of concept drift: towards formal definitions of semantic change},
  Author                   = {Antske Fokkens and Serge ter Braake and Isa Maks and Davide Ceolin},
  Booktitle                = {Proceedings of the 1st Workshop on Detection, Representation and Management of Concept Drift in Linked Open Data},
  Year                     = {2016},

  Address                  = {Aachen, Germany},
  Editor                   = {S. Dar\'anyi and L. Hollink and A. Mero\~no Pe\~nuela and E. Kontopoulos},
  Location                 = {Bologna, Italy},
  Month                    = {November},
  Organization             = {Drift-a-LOD: Detection, Representation and Management of Concept Drift in Linked Open Data},
  Pages                    = {10-17},
  Publisher                = {CEUR-WS.org},
  Series                   = {Drift-a-LOD'16},
  Volume                   = {1799},

  Abstract                 = {Semantic change and concept drift are studied in many dif-
ferent academic fields. Different domains have different understandings of
what a concept and, thus, concept drift is making it harder for researchers
to build upon work in other disciplines. In this paper, we aim to address
this challenge and propose definitions for these phenomena which apply
across fields. We provide formal definitions and illustrate how concept
drift and related phenomena can be modeled in RDF through the use
of context. We explain and support the definitions through an example
from historical research and argue that a formal modeling of semantic
change in RDF can help to better interpret data.},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/On_the_Semantics_of_Concept_Drift\:Towards_Formal_Definitions_of_Semantic_Change-Fokkens.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/On_the_Semantics_of_Concept_Drift\:Towards_Formal_Definitions_of_Semantic_Change-Fokkens.pdf:PDF},
  Url                      = {http://ceur-ws.org/Vol-1799/Drift-a-LOD2016_paper_2.pdf}
}

@Inproceedings{Fokkens2013,
  Title                    = {Offspring from reproduction problems: what replication failure teaches us},
  Author                   = {Antske Fokkens and Marieke van Erp and Marten Postma and Ted Pedersen and Piek Vossen and Nuno Freire},
  Booktitle                = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics},
  Year                     = {2013},

  Address                  = {Stroudsburg, PA, USA},
  Location                 = {Sofia, Bulgaria},
  Month                    = {August},
  Organization             = {ACL: Association of Computational Linguistics},
  Pages                    = {1691-1701},
  Publisher                = {Association for Computational Linguistics},
  Series                   = {ACL'13},

  Abstract                 = {Repeating experiments is an important in-
strument in the scientific toolbox to vali-
date previous work and build upon exist-
ing work. We present two concrete use
cases involving key techniques in the NLP
domain for which we show that reproduc-
ing results is still difficult. We show that
the deviation that can be found in repro-
duction efforts leads to questions about
how our results should be interpreted.
Moreover, investigating these deviations
provides new insights and a deeper under-
standing of the examined techniques. We
identify five aspects that can influence the
outcomes of experiments that are typically
not addressed in research papers. Our use
cases show that these aspects may change
the answer to research questions leading
us to conclude that more care should be
taken in interpreting our results and more
research involving systematic testing of
methods is required in our field.},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Offspring_from_Reproduction_Problems\:What_Replication_Failure_Teaches_Us-Fokkens.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Offspring_from_Reproduction_Problems\:What_Replication_Failure_Teaches_Us-Fokkens.pdf:PDF},
  Url                      = {https://aclweb.org/anthology/P/P13/P13-1166.pdf}
}

@Article{Gangemi2016,
  Title                    = {The Publishing Workflow Ontology (PWO)},
  Author                   = {Aldo Gangemi and Silvio Peroni and David Shotton and Fabio Vitali},
  Journal                  = {Semantic Web},
  Year                     = {2016},
  Number                   = {0},
  Pages                    = {1-12},
  Volume                   = {0},

  Abstract                 = {In this paper we introduce the Publishing Workflow Ontology (PWO), i.e., an OWL 2 DL ontology for the description of
workflows that is particularly suitable for formalising typical publishing processes such as the publication of articles in journals.
We support the presentation with a discussion of all the ontology design patterns that have been reused for modelling the main
characteristics of publishing workflows. In addition, we present two possible application of PWO in the publishing and legislative
domains.},
  Doi                      = {1570-0844/0-1900},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/The_Publishing_Workflow_Ontology-Gangemi.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/The_Publishing_Workflow_Ontology-Gangemi.pdf:PDF},
  Url                      = {http://www.semantic-web-journal.net/system/files/swj1301.pdf}
}

@Article{Groth2010,
  Title                    = {The anatomy of a nano-publication},
  Author                   = {Paul Groth and Andrew Gibson and Johannes Velterop},
  Journal                  = {Information Services and Use},
  Year                     = {2010},
  Number                   = {1-2},
  Pages                    = {51-56},
  Volume                   = {30},

  Abstract                 = {As the amount of scholarly communication increases, it is
increasingly difficult for specific core scientific statements to be
found, connected and curated. Additionally, the redundancy of
these statements in multiple fora makes it difficult to determine
attribution, quality, and provenance. To tackle these challenges,
the Concept Web Alliance has promoted the notion of
nanopublications (core scientific statements with associated
context). In this document, we present a model of
nanopublications along with a Named Graph/RDF serialization of
the model. Importantly, the serialization is defined completely
using already existing community developed technologies.
Finally, we discuss the importance of aggregating nano-
publications and the role that the Concept Wiki plays in
facilitating it.},
  Doi                      = {10.3233/ISU-2010-0613},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/The_Anatomy_of_a_Nanopublication-Groth.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/The_Anatomy_of_a_Nanopublication-Groth.pdf:PDF},
  Keywords                 = {semantic web, nanopublications},
  Url                      = {http://content.iospress.com/download/information-services-and-use/isu613?id=information-services-and-use%2Fisu613}
}

@Online{Guedon2017,
  Title                    = {Open Access: Toward the Internet of the Mind},
  Url                      = {http://www.budapestopenaccessinitiative.org/boai15/Untitleddocument.docx},

  Author                   = {Jean-Claude Guédon},
  Organization             = {Budapest Open Access Initiative},

  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Toward_the_Internet_of_the_Mind-Guedon.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Toward_the_Internet_of_the_Mind-Guedon.pdf:PDF},
  Owner                    = {Jean-Claude Guédon}
}

@Unpublished{Herian2016,
  Title                    = {Trusteeship in a post-trust world: Property, trusts law \& the blockchain},
  Author                   = {Robert Herian},
  Year                     = {2016},

  Abstract                 = {What is it that determines this progression today? We can no longer argue that it is an
economic or social condition, or education, or any other human factor. Essentially, the
preceding technical situation alone is determinative. When a given technical discovery
occurs, it has followed almost of necessity certain other discoveries. Human
intervention in this succession appears only as an incidental cause, and no man can do
this by himself. But anyone who is sufficiently up-to-date technically can make a valid
discovery which rationally follows its predecessors and rationally heralds what is to
follow},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Trusteeship_in_a_Post-Trust_World_Proper-Herian.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Trusteeship_in_a_Post-Trust_World_Proper-Herian.pdf:PDF},
  Url                      = {https://www.academia.edu/23964505/Trusteeship_in_a_Post-Trust_World_Property_Trusts_Law_and_the_Blockchain}
}

@Inproceedings{Huitfeldt2012,
  Title                    = {Documents as Timed Abstract Objects},
  Author                   = {Claus Huitfeldt and Fabio Vitali and Silvio Peroni},
  Booktitle                = {Balisage: The Markup Conference, Balisage Series on Markup Technologies},
  Year                     = {2012},
  Location                 = {Montr\'eal, Canada},
  Month                    = {August},
  Organization             = {BSMT: Balisage Series on Markup Technologies},
  Series                   = {BSMT'12},
  Volume                   = {8},

  Abstract                 = {At Balisage 2009 and 2010 Renear and Wickett discussed problems in reconciling the view that
documents are abstract objects with the fact that they can undergo change. In this paper we present an
account of documents which we believe is quite common, but which was not discussed by Renear
and Wickett.
According to this account documents are indeed abstract objects, but this is easily reconciled with the
fact that they are created and can undergo change. We then point to a similarity between this account
and the notion of so-called space-time slices. We argue that the proposed account of documents as
timed abstract objects may be subject to the same kind of criticism that has been raised against the
notion of space-time slices.
We believe that our account fares no worse than the other accounts given of documents as abstract
objects. But it still fails, and we remain agnostic about the ontological status of documents and their
relation to abstract objects, as well as about the nature of abstract objects. We conclude that either
documents are not (or not related to) abstract objects, or they are (or are related to) abstract objects of
a kind which does not correspond to the standard definition of what an abstract object is.},
  Doi                      = {0.4242/BalisageVol8.Huitfeldt01},
  File                     = {:home/cristina-iulia/git/linkflows/literature/articles/Documents_as_Timed_Abstract_Objects-Huitfeldt.pdf:PDF},
  Url                      = {https://www.balisage.net/Proceedings/vol8/html/Huitfeldt01/BalisageVol8-Huitfeldt01.html}
}

@Inproceedings{Iorio2010,
  Title                    = {Crowdsourcing semantic content: a model and two applications},
  Author                   = {Angelo Di Iorio and Alberto Musetti and Silvio Peroni and Fabio Vitali},
  Booktitle                = {Proceedings of the 2010 International Conference on Human System Interaction},
  Year                     = {2010},
  Location                 = {Rzesz\'ow, Poland},
  Month                    = {May},
  Organization             = {HSI: Human System Interaction},
  Pages                    = {563-570},
  Publisher                = {IEEE},
  Series                   = {HSI'10},

  Abstract                 = {While the original design of wikis was mainly focused on a completely open free-form text model, semantic wikis have since moved towards a more structured model for editing: users are driven to create ontological data in addition to text by using ad-hoc editing interfaces. This paper introduces OWiki, a framework for creating ontological content within not-natively-semantic wikis. Ontology-driven forms and templates are the key concepts of the system, that allows even inexpert users to create consistent semantic data with little effort. Multiple and very different instances of OWiki are presented here. The expressive power and flexibility of OWiki proved to be the right trade-off to deploy the authoring environments for such very different domains, ensuring at the same time editing freedom and semantic data consistency.},
  Doi                      = {10.1109/HSI.2010.5514513},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Crowdsourcing_Semantic_Content-A_Model_and_Two_Applications-DiIorio.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Crowdsourcing_Semantic_Content-A_Model_and_Two_Applications-DiIorio.pdf:PDF},
  Keywords                 = {ontologies},
  Url                      = {http://ieeexplore.ieee.org/document/5514513/}
}

@Inproceedings{Iorio2014,
  Title                    = {Describing bibliographic references in RDF},
  Author                   = {Angelo Di Iorio and Andrea Giovanni Nuzzolese and Silvio Peroni and David Shotton and Fabio Vitali},
  Booktitle                = {Workshop on Semantic Publishing, CEUR Workshop Proceedings},
  Year                     = {2014},
  Editor                   = {Castro, A.G. and Lange, C. and Lord, P. and Stevens, R.},
  Organization             = {ceur-ws.org},
  Pages                    = {41-56},
  Series                   = {SePublica'14},
  Volume                   = {1155},

  Abstract                 = {In this paper we present two ontologies, i.e., BiRO and C4O,
that allow users to describe bibliographic references in an accurate way,
and we introduce REnhancer, a proof-of-concept implementation of a
converter that takes as input a raw-text list of references and produces
an RDF dataset according to the BiRO and C4O ontologies.},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Describing_Bibliographic_References_in_RDF-Iorio.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Describing_Bibliographic_References_in_RDF-Iorio.pdf:PDF},
  Keywords                 = {Semantic publishing},
  Url                      = {http://ceur-ws.org/Vol-1155/paper-05.pdf}
}

@Article{Iorio2011,
  Title                    = {A Semantic Web approach to everyday overlapping markup},
  Author                   = {Angelo Di Iorio and Silvio Peroni and Fabio Vitali},
  Journal                  = {Journal of the American Society for Information Science and Technology},
  Year                     = {2011},

  Month                    = {September},
  Number                   = {9},
  Pages                    = {1696-1716},
  Volume                   = {62},

  Abstract                 = {Overlapping structures in XML are not symptoms of a misunderstanding of the intrinsic characteristics of a text document nor evidence of extreme scholarly requirements far beyond those needed by the most common XML-based applications. On the contrary, overlaps have started to appear in a large number of incredibly popular applications hidden under the guise of syntactical tricks to the basic hierarchy of the XML data format. Unfortunately, syntactical tricks have the drawback that the affected structures require complicated workarounds to support even the simplest query or usage. In this article, we present Extremely Annotational Resource Description Framework (RDF) Markup (EARMARK), an approach to overlapping markup that simplifies and streamlines the management of multiple hierarchies on the same content, and provides an approach to sophisticated queries and usages over such structures without the need of ad-hoc applications, simply by using Semantic Web tools and languages. We compare how relevant tasks (e.g., the identification of the contribution of an author in a word processor document) are of some substantial complexity when using the original data format and become more or less trivial when using EARMARK. We finally evaluate positively the memory and disk requirements of EARMARK documents in comparison to Open Office and Microsoft Word XML-based formats.},
  Doi                      = {10.1002/asi.21591},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/A_Semantic_Web_Approach_to_Everyday_Overlapping_Markup-Di_Iorio.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/A_Semantic_Web_Approach_to_Everyday_Overlapping_Markup-Di_Iorio.pdf:PDF},
  Url                      = {http://onlinelibrary.wiley.com/doi/10.1002/asi.21591/full}
}

@Article{Iorio2011a,
  Title                    = {Using semantic web technologies for analysis and validation of structural markup},
  Author                   = {Angelo Di Iorio and Silvio Peroni and Fabio Vitali},
  Journal                  = {International Journal of Web Engineering and Technology},
  Year                     = {2011},

  Month                    = {October},
  Number                   = {4},
  Pages                    = {375-398},
  Volume                   = {6},

  Abstract                 = {An increasing part of research in the Semantic Web has been
directed at making data become the main concept of the web. Plenty of
languages and specifications support this transition and work by inserting
additional (semantic) markup into web documents. Yet, little attention is being
paid to the possibility of expressing the actual structures of the documents in a
form suitable for the semantic web. EARMARK is a model for explicitly
expressing structural assertions of markup and documents, allowing a
straightforward integration of the semantics of the markup and the semantics of
the content. The well-formedness of a hierarchy, for instance, becomes an
explicit assertion and similarly the analysis of the validity of markup structures
become matter for further semantic analysis. This paper describes EARMARK
and shows a framework for using OWL ontologies, that implement particular
markup properties, to demonstrate the compliance of EARMARK documents
with those properties.},
  Doi                      = {10.1504/IJWET.2011.043439},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Using_Semantic_Web_Technologies_for_Analysis_and_Validation_of_Structural_Markup-DiIorio.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Using_Semantic_Web_Technologies_for_Analysis_and_Validation_of_Structural_Markup-DiIorio.pdf:PDF},
  Url                      = {https://www.researchgate.net/publication/220639529_Using_Semantic_Web_technologies_for_analysis_and_validation_of_structural_markup}
}

@Inproceedings{Iorio2014a,
  Title                    = {Faceted documents: describing document characteristics using semantic lenses},
  Author                   = {Angelo Di Iorio and Silvio Peroni and Fabio Vitali and Jacopo Zingoni},
  Booktitle                = {International Conference on Linked Science},
  Year                     = {2014},

  Address                  = {Aachen, Germany},
  Location                 = {Riva del Garda, Italy},
  Organization             = {LISC: Linked Science Conference},
  Pages                    = {12-23},
  Publisher                = {CEUR-WS.org},
  Series                   = {LISC'14},
  Volume                   = {1282},

  Abstract                 = {The semantic enhancement of a traditional scientific paper
is not a straightforward operation, since it involves many
different aspects or facets. In this paper we propose eight
different semantic lenses through which these facets may be
viewed, and describe and exemplify the ontologies by which
these lenses may be implemented.},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Faceted_Documents-Describing_Document_Characteristics_Using_Semantic_Lenses-Peroni.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Faceted_Documents-Describing_Document_Characteristics_Using_Semantic_Lenses-Peroni.pdf:PDF},
  Keywords                 = {semantic publishing},
  Url                      = {dl.acm.org/ft_gateway.cfm?id=2361396}
}

@Inproceedings{Kuhn2013,
  Title                    = {Broadening the Scope of Nanopublications},
  Author                   = {Tobias Kuhn and Paolo Emilio Barbano and Mate Levente Nagy and Michael Krauthammer},
  Booktitle                = {Proceedings of the 2013 European Semantic Web Conference, The Semantic Web: Semantics and Big Data, Lecture Notes in Computer Science},
  Year                     = {2013},

  Address                  = {Berlin, Heidelberg, Germany},
  Location                 = {Montpellier, France},
  Month                    = {May},
  Organization             = {ESWC: European Semantic Web Conference},
  Pages                    = {487-501},
  Publisher                = {Springer},
  Series                   = {ESWC'13},
  Volume                   = {7882},

  Abstract                 = {In this paper, we present an approach for extending the existing concept of nanopublications — tiny entities of scientific results in RDF representation — to broaden their application range. The proposed extension uses English sentences to represent informal and underspecified scientific claims. These sentences follow a syntactic and semantic scheme that we call AIDA (Atomic, Independent, Declarative, Absolute), which provides a uniform and succinct representation of scientific assertions. Such AIDA nanopublications are compatible with the existing nanopublication concept and enjoy most of its advantages such as information sharing, interlinking of scientific findings, and detailed attribution, while being more flexible and applicable to a much wider range of scientific results. We show that users are able to create AIDA sentences for given scientific results quickly and at high quality, and that it is feasible to automatically extract and interlink AIDA nanopublications from existing unstructured data sources. To demonstrate our approach, a web-based interface is introduced, which also exemplifies the use of nanopublications for non-scientific content, including meta-nanopublications that describe other nanopublications.},
  Doi                      = {10.1007/978-3-642-38288-8_33},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Broadening_the_Scope_of_Nanopublications.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Broadening_the_Scope_of_Nanopublications.pdf:PDF},
  Keywords                 = {Nanopublications, interlinking, AIDA sentences, controlled natural languages},
  Url                      = {https://link.springer.com/chapter/10.1007%2F978-3-642-38288-8_33}
}

@Article{Kuhn2016,
  Title                    = {Decentralized provenance-aware publishing with nanopublications},
  Author                   = {Tobias Kuhn and Christine Chichester and Michel Krauthammer and N\'uria Queralt-Rosinach and Ruben Verborgh and George Giannakopoulos and Axel-Cyrille Ngonga Ngomo and Raffaele Viglianti and Michel Dumontier},
  Journal                  = {PeerJ Computer Science},
  Year                     = {2016},

  Doi                      = {10.7717/peerj-cs.78},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Descentralized_Provenance-Aware_Publishing_with_Nanopublications.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Descentralized_Provenance-Aware_Publishing_with_Nanopublications.pdf:PDF},
  Keywords                 = {Data publishing, Nanopublications, Provenance, Linked Data, Semantic Web},
  Url                      = {https://peerj.com/preprints/1760/}
}

@Inproceedings{Kuhn2014,
  Title                    = {Trusty URIs: Verifiable, Immutable, and Permanent Digital Artifacts for Linked Data},
  Author                   = {Tobias Kuhn and Michel Dumontier},
  Booktitle                = {Proceedings of the 2014 European Semantic Web Conference, The Semantic Web: Trends and Challenges, Lecture Notes in Computer Science,},
  Year                     = {2014},

  Address                  = {Cham},
  Editor                   = {Presutti V. and d’Amato C. and Gandon F. and d’Aquin M. and Staab S. and Tordai A.},
  Location                 = {Anissara/Hersonissou, Crete, Greece},
  Organization             = {ESWC: European Semantic Web Conference},
  Pages                    = {395-410},
  Publisher                = {Springer},
  Series                   = {ESWC'14},
  Volume                   = {8465},

  Abstract                 = {To make digital resources on the web verifiable, immutable, and permanent, we propose a technique to include cryptographic hash values in URIs. We call them trusty URIs and we show how they can be used for approaches like nanopublications to make not only specific resources but their entire reference trees verifiable. Digital artifacts can be identified not only on the byte level but on more abstract levels such as RDF graphs, which means that resources keep their hash values even when presented in a different format. Our approach sticks to the core principles of the web, namely openness and decentralized architecture, is fully compatible with existing standards and protocols, and can therefore be used right away. Evaluation of our reference implementations shows that these desired properties are indeed accomplished by our approach, and that it remains practical even for very large files.},
  Doi                      = {10.1007/978-3-319-07443-6_27},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Trusty_URIs\:Verifiable%2CImmutable_and_Permanent_Digital_Artifacts_for_Linked_Data-Kuhn.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Trusty_URIs\:Verifiable,Immutable_and_Permanent_Digital_Artifacts_for_Linked_Data-Kuhn.pdf:PDF},
  Url                      = {https://link.springer.com/chapter/10.1007/978-3-319-07443-6_27}
}

@Article{Kuhn2015,
  Title                    = {Making Digital Artifacts on the Web Verifiable and Reliable},
  Author                   = {Tobias Kuhn and Michel Dumontier},
  Journal                  = {IEEE Transactions on Knowledge and Data Engineering},
  Year                     = {2015},

  Month                    = {September},
  Number                   = {9},
  Pages                    = {2390-2400},
  Volume                   = {27},

  Abstract                 = {The current Web has no general mechanisms to make digital artifacts-such as datasets, code, texts, and images-verifiable and permanent. For digital artifacts that are supposed to be immutable, there is moreover no commonly accepted method to enforce this immutability. These shortcomings have a serious negative impact on the ability to reproduce the results of processes that rely on Web resources, which in turn heavily impacts areas such as science where reproducibility is important. To solve this problem, we propose trusty URIs containing cryptographic hash values. We show how trusty URIs can be used for the verification of digital artifacts, in a manner that is independent of the serialization format in the case of structured data files such as nanopublications. We demonstrate how the contents of these files become immutable, including dependencies to external digital artifacts and thereby extending the range of verifiability to the entire reference tree. Our approach sticks to the core principles of the Web, namely openness and decentralized architecture, and is fully compatible with existing standards and protocols. Evaluation of our reference implementations shows that these design goals are indeed accomplished by our approach, and that it remains practical even for very large files.},
  Doi                      = {10.1109/TKDE.2015.2419657},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Making_Digital_Artifacts_on_the_Web_Verifiable_and_Reliable-Kuhn.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Making_Digital_Artifacts_on_the_Web_Verifiable_and_Reliable-Kuhn.pdf:PDF}
}

@Inproceedings{Maccatrozzo2017,
  Title                    = {SIRUP: Serendipity In Recommendations via User Perceptions},
  Author                   = {Valentina Maccatrozzo and Manon Terstall and Lora Aroyo and Guus Schreiber},
  Booktitle                = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
  Year                     = {2017},

  Address                  = {New York, NY, USA},
  Location                 = {Limassol, Cyprus},
  Organization             = {IUI: Intelligent User Interfaces},
  Pages                    = {35-44},
  Publisher                = {ACM},
  Series                   = {IUI'17},

  Abstract                 = {In this paper, we propose a model to operationalise serendipity
in content-based recommender systems. The model, called
SIRUP, is inspired by the Silvia’s curiosity theory, based on
the fundamental theory of Berlyne, aims at (1) measuring the
novelty of an item with respect to the user profile, and (2)
assessing whether the user is able to manage such level of
novelty (coping potential). The novelty of items is calculated
with cosine similarities between items, using Linked Open
Data paths. The coping potential of users is estimated by
measuring the diversity of the items in the user profile. We
deployed and evaluated the SIRUP model in a use case with
TV recommender using BBC programs dataset. Results show
that the SIRUP model allows us to identify serendipitous rec-
ommendations, and, at the same time, to have 71\% precision.},
  Doi                      = {10.1145/3025171.3025185},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/SIRUP-Serendipity_in_Recommendations_via_User_Perceptions-Maccatrozzo.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/SIRUP-Serendipity_in_Recommendations_via_User_Perceptions-Maccatrozzo.pdf:PDF},
  Url                      = {http://www.cs.vu.nl/~guus/papers/Maccatrozzo17a.pdf}
}

@Inproceedings{Mazieres1998,
  Title                    = {Escaping the evils of centralized control with self-certifying pathnames},
  Author                   = {David Mazi\`{e}res and M.Frans Kaashoek},
  Booktitle                = {European workshop on Support for composing distributed applications, ACM Special Interest Group on Operating Systems},
  Year                     = {1998},

  Address                  = {Newy York, NY, USA},
  Location                 = {Sintra, Portugal},
  Month                    = {September},
  Organization             = {ACM SIGOPS: Association of Computing Machinery Special Interest Group on Operating Systems},
  Publisher                = {ACM},
  Series                   = {ACM SIGOPS'98},

  Abstract                 = {People have long trusted central authorities to coordinate secure collaboration on local-area
networks. Unfortunately, the Internet doesn’t provide the kind of administrative structures individ-
ual organizations do. As such, users risk painful consequences if global, distributed systems rely
on central authorities for security. Fortunately, security need not come at the price of centralized
control. To prove it, we present SFS, a secure, global, decentralized file system permitting easy
cross-administrative realm collaboration. With a simple idea, self-certifying pathnames, SFS lets
users escape the evils of centralized control.},
  Doi                      = {10.1145/319195.319213},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Escaping_the_Evils_of_Centralized_Control_with_Self-certifying_Pathnames-Mazieres.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Escaping_the_Evils_of_Centralized_Control_with_Self-certifying_Pathnames-Mazieres.pdf:PDF},
  Url                      = {www.sigops.org/ew-history/1998/papers/mazieres.ps}
}

@Article{Mons2011,
  Title                    = {The value of data},
  Author                   = {Barend Mons and Herman van Haagen and Christine Chichester and Peter-Bram ‘t Hoen and Johan T den Dunnen and Gertjan van Ommen and Erik van Mulligen and Bharat Singh and Rob Hooft and Marco Roos and Joel Hammond and Bruce Kiesel and Belinda Giardine and Jan Velterop and Paul Groth and Erik Schultes},
  Journal                  = {Nature Genetics},
  Year                     = {2011},
  Number                   = {43},
  Pages                    = {281-283},

  Abstract                 = {Data citation and the derivation of semantic constructs directly from datasets have now both found their place in scientific communication. The social challenge facing us is to maintain the value of traditional narrative publications and their relationship to the datasets they report upon while at the same time developing appropriate metrics for citation of data and data constructs.},
  Doi                      = {10.1038/ng0411-281},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/The_Value_of_Data-Mons.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/The_Value_of_Data-Mons.pdf:PDF},
  Keywords                 = {Semantic publishing, data publication, data archiving, nanopublications, provenance},
  Url                      = {http://www.nature.com/ng/journal/v43/n4/full/ng0411-281.html}
}

@Article{Mons2017,
  Title                    = {Cloudy, increasingly FAIR; revisiting the FAIR Data guiding principles for the European Open Science Cloud},
  Author                   = {Barend Mons and Cameron Neylon and Jan Velterop and Michel Dumontier and Luiz Olavo Bonino da Silva Santos and Mark D. Wilkinson},
  Journal                  = {Information Services \& Use},
  Year                     = {2017},

  Month                    = {March},
  Number                   = {1},
  Pages                    = {49-65},
  Volume                   = {37},

  Doi                      = {10.3233/ISU-170824},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Cloudy%2CIncreasingly_FAIR\:Revisiting_the_FAIR_Data_guiding_Principles_for_the_European_Open_Science_Cloud-Mons.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Cloudy,Increasingly_FAIR\:Revisiting_the_FAIR_Data_guiding_Principles_for_the_European_Open_Science_Cloud-Mons.pdf:PDF},
  Url                      = {http://content.iospress.com/articles/information-services-and-use/isu824}
}

@Article{Nadeau2007,
  Title                    = {A survey of named entity recognition and classification},
  Author                   = {David Nadeau and Satoshi Sekine},
  Journal                  = {Lingvisticae Investigationes},
  Year                     = {2007},

  Month                    = {January},
  Number                   = {1},
  Pages                    = {3-26},
  Volume                   = {30},

  Doi                      = {10.1075/li.30.1.03nad},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/A_Survey_of_Named_Entity_Recognition_and_Classification-Nadeau.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/A_Survey_of_Named_Entity_Recognition_and_Classification-Nadeau.pdf:PDF},
  Url                      = {http://nlp.cs.nyu.edu/sekine/papers/li07.pdf}
}

@Article{Nuzzolese2017,
  Title                    = {Aemoo: Linked Data exploration based on knowledge patterns},
  Author                   = {Andrea Giovanni Nuzzolese and Valentina Presutti and Aldo Gangemi and Silvio Peroni and Paolo Ciancarini},
  Journal                  = {Semantic Web},
  Year                     = {2017},
  Number                   = {1},
  Pages                    = {87-112},
  Volume                   = {8},

  Abstract                 = {. This paper presents a novel approach to Linked Data exploration that uses Encyclopedic Knowledge Patterns (EKPs)
as relevance criteria for selecting, organising, and visualising knowledge. EKP are discovered by mining the linking structure of
Wikipedia and evaluated by means of a user-based study, which shows that they are cognitively sound as models for building
entity summarisations. We implemented a tool named Aemoo that supports EKP-driven knowledge exploration and integrates
data coming from heterogeneous resources, namely static and dynamic knowledge as well as text and Linked Data. Aemoo is
evaluated by means of controlled, task-driven user experiments in order to assess its usability, and ability to provide relevant and
serendipitous information as compared to two existing tools: Google and RelFinder.},
  Doi                      = {10.3233/SW-160222},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Aemoo\:%20Linked_Data_exploration_based_on_Knowledge_Patterns-Nuzzolese.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Aemoo\: Linked_Data_exploration_based_on_Knowledge_Patterns-Nuzzolese.pdf:PDF},
  Url                      = {http://www.semantic-web-journal.net/system/files/swj1208.pdf}
}

@Article{Pastorino2016,
  Title                    = {Quality assessment of studies published in Open Access and subscription Journals: results of a systematic evaluation},
  Author                   = {Roberta Pastorino and Sonja Milovanovic and Jovana Stojanovic and Ljupcho Efremov and Rosarita Amore and Stefania Boccia},
  Journal                  = {PLoS ONE},
  Year                     = {2016},

  Month                    = {May},
  Number                   = {5},
  Volume                   = {11},

  Abstract                 = {Introduction

Along with the proliferation of Open Access (OA) publishing, the interest for comparing the scientific quality of studies published in OA journals versus subscription journals has also increased. With our study we aimed to compare the methodological quality and the quality of reporting of primary epidemiological studies and systematic reviews and meta-analyses published in OA and non-OA journals.

Methods

In order to identify the studies to appraise, we listed all OA and non-OA journals which published in 2013 at least one primary epidemiologic study (case-control or cohort study design), and at least one systematic review or meta-analysis in the field of oncology. For the appraisal, we picked up the first studies published in 2013 with case-control or cohort study design from OA journals (Group A; n = 12), and in the same time period from non-OA journals (Group B; n = 26); the first systematic reviews and meta-analyses published in 2013 from OA journals (Group C; n = 15), and in the same time period from non-OA journals (Group D; n = 32). We evaluated the methodological quality of studies by assessing the compliance of case-control and cohort studies to Newcastle and Ottawa Scale (NOS) scale, and the compliance of systematic reviews and meta-analyses to Assessment of Multiple Systematic Reviews (AMSTAR) scale. The quality of reporting was assessed considering the adherence of case-control and cohort studies to STrengthening the Reporting of OBservational studies in Epidemiology (STROBE) checklist, and the adherence of systematic reviews and meta-analyses to Preferred Reporting Items for Systematic reviews and Meta-Analysis (PRISMA) checklist.

Results

Among case-control and cohort studies published in OA and non-OA journals, we did not observe significant differences in the median value of NOS score (Group A: 7 (IQR 7–8) versus Group B: 8 (7–9); p = 0.5) and in the adherence to STROBE checklist (Group A, 75\% versus Group B, 80\%; p = 0.1). The results did not change after adjustment for impact factor. The compliance with AMSTAR and adherence to PRISMA checklist were comparable between systematic reviews and meta-analyses published in OA and non-OA journals (Group C, 46.0\% versus Group D, 55.0\%; p = 0.06), (Group C, 72.0\% versus Group D, 76.0\%; p = 0.1), respectively).

Conclusion

The epidemiological studies published in OA journals in the field of oncology approach the same methodological quality and quality of reporting as studies published in non-OA journals.},
  Doi                      = {10.1371/journal.pone.0154217},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Quality_Assessment_of_Studies_Published_in_Open_Access_and_Subscription_Journals\:Results_of_a_Systematic_Evaluation-Pastorino.PDF:URL;:home/cristina-iulia/git/linkflows/literature/articles/Quality_Assessment_of_Studies_Published_in_Open_Access_and_Subscription_Journals\:Results_of_a_Systematic_Evaluation-Pastorino.PDF:PDF},
  Url                      = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0154217}
}

@Inproceedings{Patton2014,
  Title                    = {Connecting science data using semantics and information extraction},
  Author                   = {Evan W. Patton and Deborah L. McGuinness},
  Booktitle                = {Proceedings of the 4th International Conference on Linked Science},
  Year                     = {2014},

  Address                  = {Aachen, Germany},
  Location                 = {Riva del Garda, Italy},
  Organization             = {LISC: Linked Science Conference},
  Pages                    = {76-79},
  Publisher                = {CEUR-WS.org},
  Series                   = {LISC'14},
  Volume                   = {1282},

  Abstract                 = {We are developing prototypes that explicate our vision of
connecting personal medical data to scientific literature as well as to
emerging grey literature (e.g., community forums) to help people find
and understand information relevant to complex medical journeys. We
focus on robust combinations of natural language processing along with
linked data and knowledge representation to build knowledge graphs that
help people make sense of current conditions and enable new manners of
scientific hypothesis generation. We present our work in the context of a
breast cancer use case. We discuss the benefits of biomedical linked data
resources and describe some potential assistive technology for navigating
rich, diverse medical content.},
  File                     = {:home/cristina-iulia/git/linkflows/literature/articles/Connecting_Science_Data_Using_Semantics_and_Information_Extraction-Patton.pdf:PDF},
  Keywords                 = {knowledge representation, nanopublications},
  Url                      = {http://ceur-ws.org/Vol-1282/lisc2014_submission_11.pdf}
}

@PhdThesis{Peroni2012,
  Title                    = {Semantic Publishing: issues, solutions and new trends in scholarly publishing within the Semantic Web era},
  Author                   = {Silvio Peroni},
  School                   = {Universita di Bologna},
  Year                     = {2012},

  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Semantic_Publishing\:%20Issues%2CSolutions_and_New_Trends_in_Scholarly_Publishing_within_the_Semantic_Web_Era-Peroni.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Semantic_Publishing\: Issues,Solutions_and_New_Trends_in_Scholarly_Publishing_within_the_Semantic_Web_Era-Peroni.pdf:PDF},
  Url                      = {http://speroni.web.cs.unibo.it/publications/peroni-2012-semantic-publishing-issues.pdf}
}

@Inproceedings{Peroni2011,
  Title                    = {Dealing with Markup Semantics},
  Author                   = {SIlvio Peroni and Aldo Gangemi and Fabio Vitali},
  Booktitle                = {Proceedings of the International Conference on Semantic Systems},
  Year                     = {2011},

  Address                  = {New York, NY, USA},
  Location                 = {Graz, Austria},
  Month                    = {September},
  Organization             = {I-SEMANTICS: International Semantic Systems},
  Publisher                = {ACM},
  Series                   = {I-SEMANTICS '11},

  Abstract                 = {The correct interpretation of markup semantics is necessary
for the semantic interpretation of linguistic expressions that
use markup in their structuring and for enabling sophisti-
cated operation on markup documents, such as semantic
validation, multi-format document conversion and searching
on heterogeneous digital libraries. The semantics of XML-
based markup languages is usually provided informally, for
example through textual descriptions in the specification of
the language. While the syntax of XML-based languages is
entirely machine-readable, its semantics is obscure for ma-
chines. Semantic Web technologies can be useful for filling
the gap between the well-defined syntax of a language and
the informal specification of its semantics. In this paper
we show how to integrate LMM, an OWL vocabulary that
represents some core semiotic notions, with EARMARK, a
model for the specification of semantic and structural char-
acteristics of markup languages, in order to provide a better
understanding of the semantics of markup.},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Dealing_with_Markup_Semantics-Peroni.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Dealing_with_Markup_Semantics-Peroni.pdf:PDF},
  Url                      = {dl.acm.org/ft_gateway.cfm?id=2063533}
}

@Inproceedings{Peroni2008,
  Title                    = {Identifying key concepts in an ontology, through the integration of cognitive principles with statistical and topological measures},
  Author                   = {Silvio Peroni and Enrico Motta and Mathieu d’Aquin},
  Booktitle                = {Proceedings of the 2008 Asian Semantic Web Conference on The Semantic Web, Lecture Notes in Computer Science},
  Year                     = {2008},

  Address                  = {Berlin, Heidelberg},
  Month                    = {December},
  Organization             = {ASWC: Asian Semantic Web Conference},
  Pages                    = {242-256},
  Publisher                = {Springer-Verlag},
  Series                   = {ASWC'08},
  Volume                   = {5367},

  Doi                      = {10.1007/978-3-540-89704-0_17},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Identifying_Key_Concepts_in_an_Ontology_through_the_Integration_of_Cognitive_Principles_with_Statistical_and_Topological_Measures-Peroni.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Identifying_Key_Concepts_in_an_Ontology_through_the_Integration_of_Cognitive_Principles_with_Statistical_and_Topological_Measures-Peroni.pdf:PDF},
  Url                      = {http://dl.acm.org/citation.cfm?id=1484163}
}

@Article{Peroni​2016,
  Title                    = {Research Articles in Simplified HTML: a Web-first format for HTML-based scholarly articles},
  Author                   = {Silvio Peroni and Francesco Osborne and Angelo Di Iorio and Andrea Giovanni Nuzzolese and Francesco Poggi and Fabio Vitali and Enrico Motta},
  Journal                  = {PeerJ Preprints},
  Year                     = {2016},

  Month                    = {October},

  Abstract                 = {Purpose: this paper introduces the Research Articles in Simplified HTML (or RASH ), which is a Web-first format for writing HTML-based scholarly papers; it is accompanied by the RASH Framework , i.e. a set tools for interacting with RASH-based articles. The paper also presents an evaluation that involved authors and reviewers of RASH articles, submitted to the SAVE-SD 2015 and SAVE-SD 2016 workshops.

Design: RASH has been developed in order to: be easy to learn and use; share scholarly documents (and embedded semantic annotations) through the Web; support its adoption within the existing publishing workflow

Findings : the evaluation study confirmed that RASH can already be adopted in workshops, conferences and journals and can be quickly learnt by researchers who are familiar with HTML.

Research limitations: the evaluation study also highlighted some issues in the adoption of RASH, and in general of HTML formats, especially by less technical savvy users. Moreover, additional tools are needed, e.g. for enabling additional conversion from/to existing formats such as OpenXML.

Practical implications: RASH (and its Framework) is another step towards enabling the definition of formal representations of the meaning of the content of an article, facilitate its automatic discovery, enable its linking to semantically related articles, provide access to data within the article in actionable form, and allow integration of data between papers.

Social implications: RASH addresses the intrinsic needs related to the various users of a scholarly article: researchers (focussing on its content), readers (experiencing new ways for browsing it), citizen scientists (reusing available data formally defined within it through semantic annotations), publishers (using the advantages of new technologies as envisioned by the Semantic Publishing movement).

Value: RASH focuses strictly on writing the content of the paper (i.e., organisation of text + semantic annotations) and leaves all the issues about it validation, visualisation, conversion, and semantic data extraction to the various tools developed within its Framework.},
  Doi                      = {https://doi.org/10.7287/peerj.preprints.2513v1},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Research_Articles_in_Simplified_HTML\:a_Web-first_format_for_HTML-based_Scholary_Articles-Peroni.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Reasearch_Articles_in_Simplified_HTML\:a_Web-first_format_for_HTML-based_Scholary_Articles-Peroni.pdf:PDF},
  Url                      = {https://peerj.com/preprints/2513/}
}

@Article{Peroni2012a,
  Title                    = {FaBiO and CiTO: ontologies for describing bibliographic resources and citations},
  Author                   = {Silvio Peroni and David Shotton},
  Journal                  = {Web Semantics: Science, Services and Agents on the World Wide Web},
  Year                     = {2012},

  Month                    = {December},
  Pages                    = {33-43},
  Volume                   = {17},

  Abstract                 = {Semantic publishing is the use of Web and Semantic Web technologies to enhance the meaning of a published journal article, to
facilitate its automated discovery, to enable its linking to semantically related articles, to provide access to data within the article in
actionable form, and to facilitate integration of data between articles. Recently, semantic publishing has opened the possibility of a
major step forward in the digital publishing world. For this to succeed, new semantic models and visualization tools are required to
fully meet the specific needs of authors and publishers. In this article, we introduce the principles and architectures of two new
ontologies central to the task of semantic publishing: FaBiO, the FRBR-aligned Bibliographic Ontology, an ontology for recording
and publishing bibliographic records of scholarly endeavours on the Semantic Web, and CiTO, the Citation Typing Ontology, an
ontology for the characterization of bibliographic citations both factually and rhetorically. We present those two models step by step,
in order to emphasise their features and to stress their advantages relative to other pre-existing information models. Finally, we
review the uptake of FaBiO and CiTO within the academic and publishing communities.},
  Doi                      = {10.1016/j.websem.2012.08.001},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/FaBiO_and_CiTO-Ontologies_for_Describing_Bibliographic_Resources_and_Citations-Peroni.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/FaBiO_and_CiTO-Ontologies_for_Describing_Bibliographic_Resources_and_Citations-Peroni.pdf:PDF},
  Keywords                 = {semantic publishing},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1570826812000790}
}

@Article{Peroni2013a,
  Title                    = {Tools for the automatic generation of ontology documentation: a task-based evaluation},
  Author                   = {Silvio Peroni and David Shotton and Fabio Vitali},
  Journal                  = {International Journal on Semantic Web \& Information Systems},
  Year                     = {2013},

  Month                    = {January},
  Number                   = {1},
  Pages                    = {21-44},
  Volume                   = {9},

  Abstract                 = {Ontologies are knowledge constructs essential for creation of the Web of Data. Good
documentation is required to permit people to understand ontologies and thus employ them
correctly, but this is costly to create by tradition authorship methods, and is thus inefficient to create
in this way until an ontology has matured into a stable structure. We describe three tools, LODE,
Parrot and the OWLDoc-based Ontology Browser, that can be used automatically to create
documentation from a well-formed OWL ontology at any stage of its development. We contrast
their properties and we report on our evaluation of their effectiveness and usability, determined by
two task-based user testing sessions.},
  Doi                      = {10.4018/jswis.2013010102},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Tools_for_the_Automatic_Generation_of_Ontology_Documentation-A_Task-based_Evaluation-Peroni.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Tools_for_the_Automatic_Generation_of_Ontology_Documentation-A_Task-based_Evaluation-Peroni.pdf:PDF},
  Keywords                 = {ontologies},
  Url                      = {http://speroni.web.cs.unibo.it/publications/peroni-2013-tools-automatic-generation.pdf}
}

@Inproceedings{Peroni2012b,
  Title                    = {Scholarly publishing and Linked Data: describing roles, statuses, temporal and contextual extents},
  Author                   = {Silvio Peroni and David Shotton and Fabio Vitali},
  Booktitle                = {Proceedings of the International Conference on Semantic Systems},
  Year                     = {2012},

  Address                  = {New York, NY, USA},
  Location                 = {Graz, Austria},
  Month                    = {September},
  Organization             = {I-SEMANTICS: International Semantic Systems},
  Pages                    = {9-16},
  Publisher                = {ACM},
  Series                   = {I-SEMANTICS '12},

  Abstract                 = {Recently, several ontologies have been introduced for semantic publishing. However, scholarly publishing, like other real-world domains, needs to be described also in terms of precise temporal durations and the particular contexts in which the relevant processes take place. For instance, a document changes status during its publication process, e.g., from "draft" to "submitted" to "under review" to "accepted for publication", and so on. Similarly, one's roles may change with time: one's affiliation with an academic institution or one's role as a journal editor are likely to change over time. Existing well-known ontologies used to describe individuals and bibliographic entities in the Linked Data are currently not able to model situations of temporary or context-dependent possession (e.g., the holding of a status or of a role). In this paper, we address this issue by introducing two ontologies for semantic publishing, the Publishing Roles Ontology and the Publishing Status Ontology, that define the roles of people and the statuses of documents in the scholarly publishing domain.},
  Doi                      = {10.1145/2362499.2362502},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Scholarly_Publishing_and_Linked_Data-Describing_Roles_Statuses_Temporal_and_Contextual_Extents-Peroni.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Scholarly_Publishing_and_Linked_Data-Describing_Roles_Statuses_Temporal_and_Contextual_Extents-Peroni.pdf:PDF},
  Keywords                 = {semantic publishing},
  Url                      = {dl.acm.org/ft_gateway.cfm?id=2362502}
}

@Inproceedings{Peroni2013,
  Title                    = {Semantic lenses as exploration method for scholarly articles},
  Author                   = {Silvio Peroni and Francesca Tomasi and Fabio Vitali and Jacopo Zingoni},
  Booktitle                = {Italian Research Conference on Digital Libraries: Bridging Between Cultural Heritage Institutions, Communications in Computer and Information Science},
  Year                     = {2013},

  Address                  = {Berlin, Heidelberg, Germany},
  Editor                   = {Catarci T. and Ferro N. and Poggi A.},
  Publisher                = {Springer},
  Series                   = {IRCDL'13},
  Volume                   = {385},

  Abstract                 = {In a move towards an enrichment of the metadata models that are used in the electronic publication of scholarly literature, modern publishers are making steps towards semantic publishing. The possibility to explore a collection of scientific papers (a digital library, a repository or an archive of data) using different and multiple facets, i.e., different and multiple points of view on the digital collection, increases on the one hand the success of information retrieval and on the other hand the availability of richer data sets. Multiple facets are the natural navigation method made possible by an adequate ontological representation of a class of homogeneous documents. Context and content of published journal articles are thus components that in the representation of information at the metadata level constitute a fundamental approach to semantic enhancement. In this paper we introduced a test in using a particular semantic publishing model, called semantic lenses, to semantically enhance published journal articles.},
  Doi                      = {10.1007/978-3-642-54347-0_13},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Semantic_Lenses_as_Exploration_Method_for_Scholary_Articles-Peroni.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Semantic_Lenses_as_Exploration_Method_for_Scholary_Articles-Peroni.pdf:PDF},
  Keywords                 = {semantic publishing},
  Url                      = {link.springer.com/content/pdf/10.1007%252F978-3-642-54347-0_13.pdf}
}

@Inproceedings{Schneider2014,
  Title                    = {Using the Micropublications ontology and the Open Annotation Data Model to represent evidence within a drug-drug interaction knowledge base},
  Author                   = {Jodi Schneider and Paolo Ciccarese and Tim Clark and Richard D. Boyce},
  Booktitle                = {Proceedings of the 4th International Conference on Linked Science},
  Year                     = {2014},

  Address                  = {Aachen, Germany},
  Location                 = {Riva del Garda, Italy},
  Organization             = {LISC: Linked Science Conference},
  Pages                    = {60-70},
  Publisher                = {CEUR-WS.org},
  Series                   = {LISC'14},
  Volume                   = {1282},

  Abstract                 = {Semantic web technologies can support the rapid and trans-
parent validation of scientific claims by interconnecting the assumptions
and evidence used to support or challenge assertions. One important ap-
plication domain is medication safety, where more efficient acquisition,
representation, and synthesis of evidence about potential drug-drug in-
teractions is needed. Potential drug-drug interactions (PDDIs), defined
as two or more drugs for which an interaction is known to be possible,
are a significant source of preventable drug-related harm. The combi-
nation of poor quality evidence on PDDIs, and a general lack of PDDI
knowledge by prescribers, results in many thousands of preventable med-
ication errors each year. While many sources of PDDI evidence exist to
help improve prescriber knowledge, they are not concordant in their cov-
erage, accuracy, and agreement. The goal of this project is to research
and develop core components of a new model that supports more effi-
cient acquisition, representation, and synthesis of evidence about poten-
tial drug-drug interactions. Two Semantic Web models—the Micropub-
lications Ontology and the Open Annotation Data Model—have great
potential to provide linkages from PDDI assertions to their supporting
evidence: statements in source documents that mention data, materials,
and methods. In this paper, we describe the context and goals of our
work, propose competency questions for a dynamic PDDI evidence base,
outline our new knowledge representation model for PDDIs, and discuss
the challenges and potential of our approach.},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Using_the_Micropublications_Ontology_and_the_Open_Annotation_Data_Model_to_Represent_Evidence_within_a_Drug-Drug_Interaction_Knowledge_Base-Schneider.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Using_the_Micropublications_Ontology_and_the_Open_Annotation_Data_Model_to_Represent_Evidence_within_a_Drug-Drug_Interaction_Knowledge_Base-Schneider.pdf:PDF},
  Keywords                 = {Linked Data},
  Url                      = {http://ceur-ws.org/Vol-1282/lisc2014_submission_8.pdf}
}

@Manual{Shotton,
  Title                    = {Semantic annotation of publication entities},
  Author                   = {David Shotton and Silvio Peroni}
}

@Inproceedings{Sigarchian2014,
  Title                    = {EPUB3 for integrated and customizable representation of a scientific publication and its associated resources},
  Author                   = {Hajar Ghaem Sigarchian and Ben De Meester and Tom De Nies and Ruben Verborgh and Wesley De Neve and Erik Mannens and Rik Van de Walle},
  Booktitle                = {Proceedings of the 4th International Conference on Linked Science},
  Year                     = {2014},

  Address                  = {Aachen, Germany},
  Location                 = {Riva del Garda, Italy},
  Organization             = {LISC: Linked Science Conference},
  Pages                    = {1-11},
  Publisher                = {cEUR-WS.org},
  Series                   = {LISC'14},
  Volume                   = {1282},

  Abstract                 = {Scientific publications point to many associated resources,
including videos, prototypes, slides, and datasets. However, discover-
ing and accessing these resources is not always straightforward: links
could be broken, readers may be offline, or the number of associated
resources might make it difficult to keep track of the viewing order. In
this paper, we explore potential integration of such resources into the
digital version of a scientific publication. Specifically, we evaluate the
most common scientific publication formats in terms of their capability
to implement the desirable attributes of an enhanced publication and to
meet the functional goals of an enhanced publication information sys-
tem: PDF, HTML, EPUB2, and EPUB3. In addition, we present an
EPUB3 version of an exemplary publication in the field of computer
science, integrating and interlinking an explanatory video and an inter-
active prototype. Finally, we introduce a demonstrator that is capable
of outputting customized scientific publications in EPUB3. By making
use of EPUB3 to create an integrated and customizable representation
of a scientific publication and its associated resources, we believe that
we are able to augment the reading experience of scholarly publications,
and thus the effectiveness of scientific communication.},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/EPUB3_for_Integrated_and_Customizable_Representation_of_a_Scientific_Publication_and_its_Associated_Resources-Sigarchian.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/EPUB3_for_Integrated_and_Customizable_Representation_of_a_Scientific_Publication_and_its_Associated_Resources-Sigarchian.pdf:PDF},
  Url                      = {http://ceur-ws.org/Vol-1282/lisc2014_submission_3.pdf}
}

@Article{Vlachidis2016,
  Title                    = {A knowledge-based approach to Information Extraction for semantic interoperability in the archaeology domain},
  Author                   = {Andreas Vlachidis and Douglas Tudhope},
  Journal                  = {JASIST: Journal of the Association for Information Science and Technology},
  Year                     = {2016},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {1138-1152},
  Volume                   = {67},

  Abstract                 = {The article presents a method for automatic semantic indexing of archaeological grey-literature reports using empirical (rule-based) Information Extraction techniques in combination with domain-specific knowledge organization systems. The semantic annotation system (OPTIMA) performs the tasks of Named Entity Recognition, Relation Extraction, Negation Detection, and Word-Sense Disambiguation using hand-crafted rules and terminological resources for associating contextual abstractions with classes of the standard ontology CIDOC Conceptual Reference Model (CRM) for cultural heritage and its archaeological extension, CRM-EH.

Relation Extraction (RE) performance benefits from a syntactic-based definition of RE patterns derived from domain oriented corpus analysis. The evaluation also shows clear benefit in the use of assistive natural language processing (NLP) modules relating to Word-Sense Disambiguation, Negation Detection, and Noun Phrase Validation, together with controlled thesaurus expansion.

The semantic indexing results demonstrate the capacity of rule-based Information Extraction techniques to deliver interoperable semantic abstractions (semantic annotations) with respect to the CIDOC CRM and archaeological thesauri. Major contributions include recognition of relevant entities using shallow parsing NLP techniques driven by a complimentary use of ontological and terminological domain resources and empirical derivation of context-driven RE rules for the recognition of semantic relationships from phrases of unstructured text.},
  Doi                      = {10.1002/asi.23485},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/A_Knowledge-Based_Approach%20to_Information_Extraction_for_Semantic_Interoperability_in_the_Archaeology_Domain-Vlachidis.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/A_Knowledge-Based_Approach to_Information_Extraction_for_Semantic_Interoperability_in_the_Archaeology_Domain-Vlachidis.pdf:PDF},
  Url                      = {http://onlinelibrary.wiley.com/doi/10.1002/asi.23485/full}
}

@Inproceedings{Waard2012,
  Title                    = {Formalising uncertainty: an ontology of reasoning, certainty and attribution (ORCA)},
  Author                   = {Anita de Waard and Jodi Schneider},
  Booktitle                = {Proceedings of the 2012 Joint International Conference on Semantic Technologies Applied to Biomedical Informatics and Individualized Medicine},
  Year                     = {2012},

  Address                  = {Aachen, Germany},
  Editor                   = {Alejandro Rodriguez Gonzalez Centre for Plant Biotechnology and Genomics, Polytechnic University of Madrid and Jyotishman Pathak Mayo Clinic and Mark Wilkinson Centre for Plant Biotechnology \& Genomics, Polytechnic University of Madrid and Nigam Shah Stanford Center for Biomedical Informatics Research, Stanford University and Robert Stevens Department of Computer Science, University of Manchester},
  Location                 = {Boston, MA, U.S.A.},
  Month                    = {November},
  Pages                    = {10-17},
  Publisher                = {CEUR-WS.org},
  Series                   = {SATBI+SWIM'12},
  Volume                   = {930},

  Abstract                 = {To enable better representations of biomedical argumentation over collections of research papers, we propose a model and a lightweight ontology to represent interpersonal, discourse-based, data-driven reasoning. This model is applied to a collection of scientific documents, to show how it can be applied in practice. We present three biomedical applications for this work, and suggest connections with other, existing, ontologies and reasoning tools. Specifically, this model offers a lightweight way to connect nanopublication-like formal representations to scientific papers written in natural language.},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Formalising_Uncertainty-An_Ontology_of_Reasoning_Certainty_and_Attribution-deWaard.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Formalising_Uncertainty-An_Ontology_of_Reasoning_Certainty_and_Attribution-deWaard.pdf:PDF},
  Keywords                 = {scholarly communication, ontologies, nanopublications, semantic publishing, provenance trails},
  Url                      = {http://dl.acm.org/citation.cfm?id=2887634}
}

@Article{Weale2004,
  Title                    = {The level of non-citation of articles within a journal as a measure of quality: a comparison to the impact factor},
  Author                   = {Andy R Weale and Mick Bailey and Paul A Lear},
  Journal                  = {BMC Medical Research Methodology},
  Year                     = {2004},

  Month                    = {May},
  Number                   = {14},
  Volume                   = {4},

  Abstract                 = {Background
Current methods of measuring the quality of journals assume that citations of articles within journals are normally distributed. Furthermore using journal impact factors to measure the quality of individual articles is flawed if citations are not uniformly spread between articles. The aim of this study was to assess the distribution of citations to articles and use the level of non-citation of articles within a journal as a measure of quality. This ranking method is compared with the impact factor, as calculated by ISI.

Methods
Total citations gained by October 2003, for every original article and review published in current immunology (13125 articles; 105 journals) and surgical (17083 articles; 120 journals) fields during 2001 were collected using ISI Web of Science.

Results
The distribution of citation of articles within an individual journal is mainly non-parametric throughout the literature. One sixth (16.7\%; IQR 13.6–19.2) of articles in a journal accrue half the total number of citations to that journal. There was a broader distribution of citation to articles in higher impact journals and in the field of immunology compared to surgery. 23.7\% (IQR 14.6–42.4) of articles had not yet been cited. Levels of non-citation varied between journals and subject fields. There was a significant negative correlation between the proportion of articles never cited and a journal's impact factor for both immunology (rho = -0.854) and surgery journals (rho = -0.924).

Conclusion
Ranking journals by impact factor and non-citation produces similar results. Using a non-citation rate is advantageous as it creates a clear distinction between how citation analysis is used to determine the quality of a journal (low level of non-citation) and an individual article (citation counting). Non-citation levels should therefore be made available for all journals.},
  Doi                      = {10.1186/1471-2288-4-14},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/The_Level_of_Non-citation_of_Articles_within_a_Journal_as_a_Measure_of_quality\:A_Comparison_to_the_Impact_Factor-Weale.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/The_Level_of_Non-citation_of_Articles_within_a_Journal_as_a_Measure_of_quality\:A_Comparison_to_the_Impact_Factor-Weale.pdf:PDF},
  Url                      = {https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-4-14}
}

@Article{Wilkinson2016,
  Title                    = {The FAIR Guiding Principles for scientific data management and stewardship},
  Author                   = {Mark D. Wilkinson and Michel Dumontier and IJsbrand Jan Aalbersberg and Gabrielle Appleton and Myles Axton and Arie Baak and Niklas Blomberg and Jan-Willem Boiten and Luiz Bonino da Silva Santos and Philip E. Bourne and Jildau Bouwman and Anthony J. Brookes and Tim Clark and Mercè Crosas and Ingrid Dillo and Olivier Dumon and Scott Edmunds and Chris T. Evelo and Richard Finkers and Alejandra Gonzalez-Beltran and Alasdair J.G. Gray and Paul Groth and Carole Goble and Jeffrey S. Grethe and Jaap Heringa and Peter A.C ’t Hoen and Rob Hooft and Tobias Kuhn and Ruben Kok and Joost Kok and Scott J. Lusher and Maryann E. Martone and Albert Mons and Abel L. Packer and Bengt Persson and Philippe Rocca-Serra and Marco Roos and Rene van Schaik and Susanna-Assunta Sansone and Erik Schultes and Thierry Sengstag and Ted Slater and George Strawn and Morris A. Swertz and Mark Thompson and Johan van der Lei and Erik van Mulligen and Jan Velterop and Andra Waagmeester and Peter Wittenburg and Katherine Wolstencroft and Jun Zhao and Barend Mons},
  Journal                  = {Scientific Data},
  Year                     = {2016},

  Month                    = {March},
  Volume                   = {3},

  Abstract                 = {There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders—representing academia, industry, funding agencies, and scholarly publishers—have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community.},
  Doi                      = {10.1038/sdata.2016.18},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/The_FAIR_Guiding_Principles_for_Scientific_Data_Management_and_Stewardship-Wilkinson.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/The_FAIR_Guiding_Principles_for_Scientific_Data_Management_and_Stewardship-Wilkinson.pdf:PDF},
  Url                      = {https://www.nature.com/articles/sdata201618}
}

@Inproceedings{Zheng2014,
  Title                    = {Entity Linking for Biomedical Literature},
  Author                   = {Jin Guang Zheng and Daniel Howsmon and Boliang Zhang and Juergen Hahn and Deborah McGuinness and James Hendler and Heng Ji},
  Booktitle                = {Proceedings of the ACM 8th International Workshop on Data and Text Mining in Bioinformatics},
  Year                     = {2014},

  Address                  = {New York, NY, USA},
  Location                 = {Shangai, China},
  Month                    = {November},
  Organization             = {DTMBIO: Data and Text Mining in Bioinformatics},
  Publisher                = {ACM},
  Series                   = {DTMBIO'14},

  Abstract                 = {The Entity Linking (EL) task links entity mentions from an unstructured document to entities in a knowledge base. Although this problem is well-studied in news and social media, this problem has not received much attention in the life science domain. One outcome of tackling the EL problem in the life sciences domain is to enable scientists to build computational models of biological processes with more efficiency. However, simply applying a news-trained entity linker produces inadequate results. Since existing supervised approaches require a large amount of manually-labeled training data, which is currently unavailable for the life science domain, we propose a novel unsupervised collective inference approach to link entities from unstructured full texts of biomedical literature to 300 ontologies. The approach leverages the rich semantic information and structures in ontologies for similarity computation and entity ranking.

Without using any manual annotation, our approach significantly outperforms state-of-the-art supervised EL method (9\% absolute gain in linking accuracy). Furthermore, the state-of-the-art supervised EL method requires 15,000 manually annotated entity mentions for training. These promising results establish a benchmark for the EL task in the life science domain1. We also provide in depth analysis and discussion on both challenges and opportunities on automatic knowledge enrichment for scientific literature.

In this paper, we propose a novel unsupervised collective inference approach to address the EL problem in a new domain. We show that our unsupervised approach is able to outperform a current state-of-the-art supervised approach that has been trained with a large amount of manually labeled data. Life science presents an underrepresented domain for applying EL techniques. By providing a small benchmark data set and identifying opportunities, we hope to stimulate discussions across natural language processing and bioinformatics and motivate others to develop techniques for this largely untapped domain.},
  Doi                      = {10.1145/2665970.2665974},
  File                     = {:https\://github.com/LaraHack/linkflows/blob/master/literature/articles/Entity_Linking_for_Biomedical_Literature-Zheng.pdf:URL;:home/cristina-iulia/git/linkflows/literature/articles/Entity_Linking_for_Biomedical_Literature-Zheng.pdf:PDF},
  Url                      = {http://dl.acm.org/ft_gateway.cfm?id=2665974&ftid=1510757&dwn=1&CFID=921111123&CFTOKEN=39179302}
}

@Online{altmetric,
  Title                    = {Altmetric},
  Url                      = {https://www.altmetric.com},

  Abstract                 = {A single research output may live online in multiple websites and can be talked about across dozens of different platforms. At Altmetric, we work behind the scenes, collecting and collating all of this disparate information to provide you with a single visually engaging and informative view of the online activity surrounding your scholarly content.},
  Addendum                 = {\\Aggregator of online presence mentions of scholary content by monitoring: mainstream media outlets like news feeds, online reference managers like Mendeley, data from Open Syllabus Project, research highlights from F1000, Wikipedia, blogs, citations, social media like Facebook (public pages), Twitter, Google+, LinkedIn, etc. together with multimedia and other online platforms like YouTube, Reddit, etc.\\
 Tools offered: browser plugin to see altimetrics for any publication with a DOI, embedding altimetrics statistics, altimetrics API to query the database.},
  HowPublished             = {2017-02-21},
  Keywords                 = {interlinking mentions of scientific resources, impact of research, flavours of impact, monitor research},
  Review                   = {Acesta este un test},
  Timestamp                = {2017-02-21}
}

@Online{bibsonomy,
  Title                    = {BibSonomy},
  Url                      = {https://www.bibsonomy.org},

  Abstract                 = {BibSonomy helps you to organize your scientific work. Use BibSonomy to collect publications and bookmarks, to collaborate with your colleagues, and to discover interesting researches for your daily work.},
  Addendum                 = {\\An online framework that allows users to collect and bookmark publications for their own usage and also tag them. This collection of publications can then be easily used to create a bibliography in various formats. Also, users can connect to each other and collaborate.},
  Keywords                 = {semantic publishing, collection of publications}
}

@Online{connectedresearchers,
  Title                    = {ConnectedResearchers},
  Url                      = {http://connectedresearchers.com/online-tools-for-researchers/},

  Addendum                 = {\\A list with digital tools for researchers categorized as follows: literature exploration, code and data sharing, connecting with others, writing and publishing articles and evaluating the research.},
  Keywords                 = {online tools, research, semantic publishing, quality assessment}
}

@Online{dqv,
  Title                    = {Data Quality Vocabulary},
  Url                      = {https://www.w3.org/TR/vocab-dqv/},

  Abstract                 = {A framework in which the quality of a dataset can be described, whether by the dataset publisher or by a broader community of users. It does not provide a formal, complete definition of quality, rather, it sets out a consistent means by which information can be provided such that a potential user of a dataset can make his/her own judgment about its fitness for purpose.},
  Addendum                 = {\\A W3C framework that allows the description of the quality of a dataset. The quality assessments can be made by various organizations and/or people: certification aggencies, data aggregators, data consumers, etc.},
  Keywords                 = {quality assessment}
}

@Online{hypothes,
  Title                    = {Hypothes},
  Url                      = {https://hypothes.is/},

  Addendum                 = {\\An open platform that uses annotations and notes to share regarding news, blogs, scientific articles, books, etc.}
}

@Online{labworm,
  Title                    = {LabWorm},
  Url                      = {https://labworm.com/},

  Addendum                 = {\\Search engine for online tools to assist in research},
  Keywords                 = {semantic publishing, search engine}
}

@Online{ldn,
  Title                    = {Linked Data Notifications},
  Url                      = {https://www.w3.org/TR/ldn/},

  Abstract                 = {Linked Data Notifications is a protocol that describes how servers (receivers) can have messages pushed to them by applications (senders), as well as how other applications (consumers) may retrieve those messages. Any resource can advertise a receiving endpoint (Inbox) for the messages. Messages are expressed in RDF, and can contain any data.},
  Addendum                 = {\\A W3C protocol that allows data to be shared across applications, independent of the data storage.},
  Keywords                 = {linking, semantic publishing, W3C protocol}
}

@Online{provpings,
  Title                    = {PROV-Pings},
  Url                      = {http://prov-pings.org},

  Abstract                 = {PROV-Pings is a service that allows you to link publications to their provenance, and query and retrieve it afterwards. It combines a provenance query and pingback service.},
  Addendum                 = {\\A web service used to link publications to their provenance. The responsibility of providing a provenance to a publication is not in the hands of the publisher anymore, but in the hands of the crowd. This is done by pushing a query with the provenance information.\\
PROV-AQ (Provenance Access and Query):\\
\textless publication-uri\textgreater prov:has\_provenance <provenance-uri>\\
PROV-Pings:\\
<publication-uri> prov:pingback <prov-pings-uri?target=publication-uri>\\
<publication-uri> prov:has\_query\_service <prov-pings-uri>},
  Keywords                 = {semantic publishing, provenance, linking}
}

@Online{semanticscholar,
  Title                    = {Semantic Scholar},
  Url                      = {https://www.semanticscholar.org/},

  Addendum                 = {\\A corpus of computer science and neuroscience papers with a search engine for the publications in its database. It can also identify missing citations using Citeomatic.},
  Keywords                 = {semantic pubishing, quality assessment, search engine}
}

@comment{jabref-entrytype: Online: req[title;url] opt[author;organization]}

